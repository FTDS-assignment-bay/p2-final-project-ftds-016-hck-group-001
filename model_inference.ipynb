{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fredericksembiring/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fredericksembiring/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Input, Dropout\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "# for text preprocess\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# save model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_1 = load_model('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load TextVectorization model (digunakan jika local tidak bisa masukin textvectorization ke model)\n",
    "\n",
    "# vectorization_data = pickle.load(open('vectorizer.pkl', 'rb'))\n",
    "# vectorizer = TextVectorization.from_config(vectorization_data['config'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  date_received          product    sub_product  \\\n",
      "0    2018-07-29  Debt collection        Medical   \n",
      "1    2020-03-20  Debt collection    Credit card   \n",
      "2    2022-04-19  Debt collection  I do not know   \n",
      "\n",
      "                                   issue                          sub_issue  \\\n",
      "0  Cont'd attempts collect debt not owed                      Debt was paid   \n",
      "1     False statements or representation  Attempted to collect wrong amount   \n",
      "2                  Communication tactics         Frequent or repeated calls   \n",
      "\n",
      "                        consumer_complaint_narrative  \\\n",
      "0  I do not own anymore debt from this bank anymo...   \n",
      "1  Why am I must pay double the amount of my bill...   \n",
      "2  STOP CALLING ME FOR NO REASON! I REALLY DESPIS...   \n",
      "\n",
      "                             company_public_response                 company  \\\n",
      "0  We're sorry to hear that. We will communicate ...         Genesis Lending   \n",
      "1  We're sorry to hear that. We will communicate ...                  Paypal   \n",
      "2  Hi, sorry to hear that. We will evaluate our p...  Roquemore Holdings LLC   \n",
      "\n",
      "  state zip_code            tags consumer_consent_provided? submitted_via  \\\n",
      "0    AE    092XX  Older American           Consent provided           Web   \n",
      "1    NJ    076XX  Older American           Consent provided           Web   \n",
      "2    NY    146XX  Older American           Consent provided           Web   \n",
      "\n",
      "  date_sent_to_company company_response_to_consumer timely_response?  \\\n",
      "0           2018-07-12                       Closed              Yes   \n",
      "1           2020-03-10                       Closed              Yes   \n",
      "2           2022-04-10                       Closed              Yes   \n",
      "\n",
      "  consumer_disputed?  complaint_id  \n",
      "0                 No       1807128  \n",
      "1                 No       2003104  \n",
      "2                 No       2204107  \n"
     ]
    }
   ],
   "source": [
    "# Assuming data_dummy is your dictionary\n",
    "data_dummy = {\n",
    "    'date_received': [\n",
    "        '2018-07-29', '2020-03-20', '2022-04-19'\n",
    "    ],\n",
    "    'product': [\n",
    "        'Debt collection', 'Debt collection', 'Debt collection'\n",
    "    ],\n",
    "    'sub_product': [\n",
    "        'Medical', 'Credit card', 'I do not know'\n",
    "    ],\n",
    "    'issue': [\n",
    "        \"Cont'd attempts collect debt not owed\", \"False statements or representation\", \"Communication tactics\"\n",
    "    ],\n",
    "    'sub_issue': [\n",
    "        \"Debt was paid\", \"Attempted to collect wrong amount\", \"Frequent or repeated calls\"\n",
    "    ],\n",
    "    'consumer_complaint_narrative': [\n",
    "        \"I do not own anymore debt from this bank anymore. Please dont send me any more debt bill. \", \"Why am I must pay double the amount of my bill. This is a harrasment and a scam in progress!\", \"STOP CALLING ME FOR NO REASON! I REALLY DESPISE THIS KIND OF MARKETING TACTICS!\"\n",
    "    ],\n",
    "    'company_public_response': [\n",
    "        \"We're sorry to hear that. We will communicate with billing division to solve this problem.\", \"We're sorry to hear that. We will communicate with billing division to check for this problem\", \"Hi, sorry to hear that. We will evaluate our program to suit best for our customer's need. Thank you\"\n",
    "    ],\n",
    "    'company': [\n",
    "        \"Genesis Lending\", \"Paypal\", \"Roquemore Holdings LLC\"\n",
    "    ],\n",
    "    'state': [\n",
    "        \"AE\", \"NJ\", \"NY\"\n",
    "    ],\n",
    "    'zip_code': [\n",
    "        \"092XX\", \"076XX\", \"146XX\"\n",
    "    ],\n",
    "    'tags': [\n",
    "        \"Older American\", \"Older American\", \"Older American\"\n",
    "    ],\n",
    "    'consumer_consent_provided?': [\n",
    "        \"Consent provided\", \"Consent provided\", \"Consent provided\"\n",
    "    ],\n",
    "    'submitted_via': [\n",
    "        \"Web\", \"Web\", \"Web\"\n",
    "    ],\n",
    "    'date_sent_to_company': [\n",
    "        \"2018-07-12\", \"2020-03-10\", \"2022-04-10\"\n",
    "    ],\n",
    "    'company_response_to_consumer': [\n",
    "        \"Closed\", \"Closed\", \"Closed\"\n",
    "    ],\n",
    "    'timely_response?': [\n",
    "        \"Yes\", \"Yes\", \"Yes\"\n",
    "    ],\n",
    "    'consumer_disputed?': [\n",
    "        \"No\", \"No\", \"No\"\n",
    "    ],\n",
    "    'complaint_id': [\n",
    "        1807128, 2003104, 2204107\n",
    "    ],\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_data_dummy = pd.DataFrame(data_dummy)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df_data_dummy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Combined Narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine issue, sub_issue, and consumer_complaint_narrative into one column\n",
    "df_data_dummy['sentence'] = df_data_dummy['issue'] + ' ' + df_data_dummy['sub_issue'] + ' ' + df_data_dummy['consumer_complaint_narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define Stemming\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A Function for Text Preprocessing\n",
    "def text_preprocessing(text):\n",
    "  # Case folding\n",
    "  text = text.lower()\n",
    "\n",
    "  # Mention removal\n",
    "  text = re.sub(\"@[A-Za-z0-9_]+\", \" \", text)\n",
    "\n",
    "  # Hashtags removal\n",
    "  text = re.sub(\"#[A-Za-z0-9_]+\", \" \", text)\n",
    "\n",
    "  # Newline removal (\\n)\n",
    "  text = re.sub(r\"\\\\n\", \" \",text)\n",
    "\n",
    "  # Whitespace removal\n",
    "  text = text.strip()\n",
    "\n",
    "  # URL removal\n",
    "  text = re.sub(r\"http\\S+\", \" \", text)\n",
    "  text = re.sub(r\"www.\\S+\", \" \", text)\n",
    "\n",
    "  # Non-letter removal (such as emoticon, symbol (like μ, $, 兀), etc\n",
    "  text = re.sub(\"[^A-Za-z\\s']\", \" \", text)\n",
    "  text = re.sub(\"'\", \"\", text)\n",
    "\n",
    "  # Tokenization\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  # Stopwords removal\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "  # Stemming\n",
    "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "  # Combining Tokens\n",
    "  text = \" \".join(tokens)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    contd attempt collect debt owed debt paid anym...\n",
       "1    false statement representation attempted colle...\n",
       "2    communication tactic frequent repeated call st...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = df_data_dummy['sentence'].apply(lambda x: text_preprocessing(x))\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapt and set weight for TextVectorizer (digunakan jika local tidak bisa masukin textvectorization ke model)\n",
    "\n",
    "# vectorizer.adapt(sentence)\n",
    "# vectorizer.set_weights(vectorization_data['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize data before prediction (digunakan jika local tidak bisa masukin textvectorization ke model)\n",
    "\n",
    "# sentence_vectd = vectorizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "predict_proba = model_1.predict(sentence)\n",
    "\n",
    "\n",
    "predict_proba[:,0] = np.where(predict_proba[:,0] > 0.1, 1, 0)  # Thresholding light complaints at 0.1\n",
    "predict_proba[:,1] = np.where(predict_proba[:,1] > 0.1, 1, 0)  # Thresholding harsh complaints at 0.1\n",
    "predict_proba[:,2] = np.where(predict_proba[:,2] > 0.75, 1, 0)  # Thresholding mid complaints at 0.75\n",
    "\n",
    "prediction = predict_proba[0]\n",
    "print(f'Prediction: {prediction}')\n",
    "predicted_label = []\n",
    "if prediction[0] == 1:\n",
    "    predicted_label.append('light complaint')\n",
    "if prediction[1] == 1:\n",
    "    predicted_label.append('harsh complaint')\n",
    "if prediction[2] == 1:\n",
    "    predicted_label.append('mid complaint')\n",
    "\n",
    "print('label for predicted sentence')\n",
    "for x in predicted_label:\n",
    "    print(f\"-{x}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
